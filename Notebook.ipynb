{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "popular"
      ],
      "metadata": {
        "id": "OmpBsXh4Re5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the book data and ratings data into separate dataframes\n",
        "book_data = pd.read_excel('books_1.Best_Books_Ever.csv.xlsx')\n",
        "ratings_data = pd.read_csv('goodreads_interactions (1).csv')\n",
        "\n",
        "# Merge the two dataframes on the ISBN column to combine book information and ratings information\n",
        "book_ratings = pd.merge(book_data, ratings_data, on='book_id')\n",
        "\n",
        "# Group the data by ISBN and compute the average rating and number of ratings for each book\n",
        "grouped_data = book_ratings.groupby(['book_id']).agg({'rating': 'mean', 'numRatings': 'count'})\n",
        "\n",
        "# Create a popularity score for each book based on the average rating and number of ratings\n",
        "grouped_data['popularity_score'] = (grouped_data['rating'] * grouped_data['numRatings']) / (grouped_data['numRatings'] + 10)\n",
        "\n",
        "# Sort the data by popularity score in descending order and print the top 10 books\n",
        "grouped_data.sort_values(by=['popularity_score'], ascending=False, inplace=True)\n",
        "print(grouped_data.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUgApo_YRcZn",
        "outputId": "45774a0b-3f29-407f-f662-584824d10a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         rating  numRatings  popularity_score\n",
            "book_id                                      \n",
            "1408       4.85         238          4.654435\n",
            "7351       4.79         321          4.645287\n",
            "1000       4.59         844          4.536253\n",
            "410        4.73         207          4.512028\n",
            "21897      4.93         103          4.493717\n",
            "19060      4.75         152          4.456790\n",
            "1601       4.68         178          4.431064\n",
            "5239       4.57         314          4.428951\n",
            "759        4.48         487          4.389859\n",
            "19964      4.75         108          4.347458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "genres"
      ],
      "metadata": {
        "id": "2rAfcqFORavX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the books excel file\n",
        "books_df = pd.read_excel('books_1.Best_Books_Ever.csv.xlsx')\n",
        "\n",
        "# get user's favorite genres\n",
        "user_fav_genres = input('Enter your favorite genres (separated by commas): ').split(',')\n",
        "\n",
        "# use content-based filtering to filter books based on matching genres\n",
        "genre_cols = ['genre 1', 'genre 2', 'genre 3', 'genre 4','genre 5']\n",
        "matched_books_df = books_df[books_df[genre_cols].isin(user_fav_genres).any(axis=1)]\n",
        "genre_counts = matched_books_df[genre_cols].apply(lambda x: sum(x.isin(user_fav_genres)), axis=1)\n",
        "matched_books_df = matched_books_df[genre_counts >= 2].reset_index(drop=True)\n",
        "\n",
        "# use collaborative filtering to filter out books already read and rated highly by the user\n",
        "user_df = pd.read_csv('goodreads_interactions (1).csv')\n",
        "user_ratings_df = user_df[user_df['Rating'] >= 4]  # consider only high ratings\n",
        "user_rated_book_ids = user_ratings_df['book_id'].unique()\n",
        "collab_filtered_books_df = matched_books_df[~matched_books_df['book_id'].isin(user_rated_book_ids)]\n",
        "\n",
        "# calculate weighted score for the remaining books\n",
        "C = books_df['rating'].mean()\n",
        "m = books_df['numRatings'].quantile(0.9)\n",
        "collab_filtered_books_df['weighted_score'] = (collab_filtered_books_df['numRatings'] / (collab_filtered_books_df['numRatings'] + m) * collab_filtered_books_df['rating']) + (m / (collab_filtered_books_df['numRatings'] + m) * C)\n",
        "\n",
        "# sort the books by weighted score and display top 10 recommendations\n",
        "collab_filtered_books_df = collab_filtered_books_df.sort_values('weighted_score', ascending=False).reset_index(drop=True)\n",
        "print(f\"Recommended books for {', '.join(user_fav_genres)}:\\n\")\n",
        "for i, book in collab_filtered_books_df.head(10).iterrows():\n",
        "    print(f\"{i+1}. {book['book_id']} ({book['title']}) - {book['weighted_score']:.2f} weighted score, {book['likedPercent']}% liked\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eorwcnktQ_im",
        "outputId": "147ec4f8-3cfe-4c0c-9eb2-2fbd010626b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your favorite genres (separated by commas): Comedy,Romance,Classics\n",
            "Recommended books for Comedy, Romance, Classics:\n",
            "\n",
            "1. 4 (Pride and Prejudice) - 4.26 weighted score, 94.0% liked\n",
            "2. 357 (The Complete Novels) - 4.24 weighted score, 97.0% liked\n",
            "3. 16 (Memoirs of a Geisha) - 4.12 weighted score, 95.0% liked\n",
            "4. 10183 (Carry On, Jeeves) - 4.09 weighted score, 98.0% liked\n",
            "5. 70 (Little Women) - 4.09 weighted score, 93.0% liked\n",
            "6. 15021 (The Inimitable Jeeves) - 4.08 weighted score, 98.0% liked\n",
            "7. 1462 (Katherine) - 4.08 weighted score, 95.0% liked\n",
            "8. 5855 (Leave It to Psmith) - 4.06 weighted score, 98.0% liked\n",
            "9. 538 (Much Ado About Nothing) - 4.05 weighted score, 94.0% liked\n",
            "10. 437 (The Scarlet Pimpernel) - 4.05 weighted score, 94.0% liked\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1260f294170d>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  collab_filtered_books_df['weighted_score'] = (collab_filtered_books_df['numRatings'] / (collab_filtered_books_df['numRatings'] + m) * collab_filtered_books_df['rating']) + (m / (collab_filtered_books_df['numRatings'] + m) * C)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CF auto"
      ],
      "metadata": {
        "id": "4JjlT87FQ7VQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUN3U6r7l4VQ",
        "outputId": "de3774a9-a995-435d-fca3-dd5523420306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 20s 880ms/step - loss: 0.3446 - val_loss: 0.2906\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 10s 775ms/step - loss: 0.2738 - val_loss: 0.2501\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 10s 785ms/step - loss: 0.1461 - val_loss: 0.0158\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 10s 785ms/step - loss: 0.0125 - val_loss: 0.0093\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 10s 763ms/step - loss: 0.0077 - val_loss: 0.0058\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 10s 717ms/step - loss: 0.0047 - val_loss: 0.0035\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 10s 783ms/step - loss: 0.0030 - val_loss: 0.0022\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 10s 788ms/step - loss: 0.0019 - val_loss: 0.0014\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 10s 787ms/step - loss: 0.0013 - val_loss: 9.7044e-04\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 10s 782ms/step - loss: 9.1365e-04 - val_loss: 6.9742e-04\n",
            "64/64 [==============================] - 6s 76ms/step\n",
            "MSE: 0.0003807988092766426\n",
            "MAE: 0.0003807988\n",
            "RMSE: 0.01951406695890538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f2e46163ce55>:87: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, GRU, LSTM\n",
        "from keras.models import Model\n",
        "from keras import regularizers, optimizers\n",
        "from keras.metrics import mean_absolute_error\n",
        "from keras.metrics import MeanSquaredError\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, GRU, LSTM, SeparableConv1D\n",
        "\n",
        "books_df = pd.read_excel('books_1.Best_Books_Ever.csv.xlsx')\n",
        "users_df = pd.read_csv('goodreads_interactions (1).csv')\n",
        "\n",
        "# Merge dataframes\n",
        "merged_df = pd.merge(users_df, books_df, on='book_id')\n",
        "\n",
        "# One-hot encode genre\n",
        "genres = pd.get_dummies(merged_df['genre 1'])\n",
        "merged_df = pd.concat([merged_df, genres], axis=1)\n",
        "\n",
        "# Create user-item matrix\n",
        "user_item_matrix = pd.pivot_table(merged_df, values='is_reviewed', index='user_id', columns='book_id')\n",
        "user_item_matrix = user_item_matrix.fillna(0)\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(user_item_matrix)\n",
        "\n",
        "# Define autoencoder model\n",
        "input_layer = Input(shape=(user_item_matrix.shape[1],))\n",
        "encoded = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(input_layer)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "encoded = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(encoded)\n",
        "encoded = Dropout(0.5)(encoded)\n",
        "\n",
        "encoded = encoded[:, :, np.newaxis]  # add new axis for channels\n",
        "encoded = SeparableConv1D(64, 3, activation='relu', padding='same')(encoded)\n",
        "\n",
        "#encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling1D(2)(encoded)\n",
        "encoded = GRU(32, activation='relu', return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(encoded)\n",
        "encoded = LSTM(32, activation='relu', dropout=0.2, recurrent_dropout=0.2)(encoded)\n",
        "\n",
        "encoded = encoded[:, :, np.newaxis]  # add new axis for channels\n",
        "encoded = SeparableConv1D(64, 3, activation='relu', padding='same')(encoded)\n",
        "\n",
        "encoded = Conv1D(32, 5, activation='relu', padding='same')(encoded)\n",
        "encoded = MaxPooling1D(2)(encoded)\n",
        "\n",
        "encoded = GRU(32, activation='relu', return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(encoded)\n",
        "encoded = LSTM(32, activation='relu', dropout=0.2, recurrent_dropout=0.2)(encoded)\n",
        "\n",
        "\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(256, activation='relu')(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(512, activation='relu')(decoded)\n",
        "decoded = Dropout(0.5)(decoded)\n",
        "decoded = Dense(user_item_matrix.shape[1], activation='sigmoid')(decoded)\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "# Compile autoencoder model\n",
        "optimizer = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Train autoencoder\n",
        "history = autoencoder.fit(user_item_matrix, user_item_matrix, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "\n",
        "\n",
        "# Predict filtered matrix\n",
        "filtered_matrix = autoencoder.predict(user_item_matrix)\n",
        "\n",
        "filtered_matrix = np.nan_to_num(filtered_matrix)\n",
        "filtered_cosine_sim = cosine_similarity(filtered_matrix)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "# Calculate MSE, MAE, and MAPE\n",
        "mse = mean_squared_error(user_item_matrix, filtered_matrix)\n",
        "mae = mean_absolute_error(user_item_matrix, filtered_matrix)\n",
        "mape = mean_absolute_percentage_error(user_item_matrix, filtered_matrix)\n",
        "print(\"MSE:\", mse)\n",
        "#print(\"MAE:\", mae)\n",
        "#print(\"MAPE:\", mape)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "mae_value = np.mean(mae.numpy())\n",
        "print(\"MAE:\", mae_value)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "\n",
        "\n",
        "\n",
        "import random\n",
        "def recommend_books(user_id, num_recommendations=50):\n",
        "    user_row = user_item_matrix.loc[user_id]\n",
        "    user_similarities = pd.Series(cosine_sim[user_id])\n",
        "    top_users = user_similarities.sort_values(ascending=False)[1:num_recommendations+1].index\n",
        "    top_users_filtered = filtered_matrix[top_users]\n",
        "    weighted_ratings = np.dot(top_users_filtered, user_row) / np.sum(top_users_filtered, axis=1)\n",
        "    books_read = user_row[user_row > 0].index\n",
        "    books_unread = books_df[~books_df['book_id'].isin(books_read)]['book_id']\n",
        "    recommended_books = [book_id for book_id in books_unread if book_id in books_df.index]\n",
        "    recommended_books = random.sample(recommended_books, min(num_recommendations, len(recommended_books)))\n",
        "    return books_df.loc[recommended_books]\n",
        "\n",
        "\n",
        "    recommendations = recommend_books(user_id=500, num_recommendations=10)\n",
        "    print(recommendations)\n",
        "\n",
        "\n",
        "  # Define book recommendation function\n",
        "    #def recommend_books(user_id, num_recommendations=50):\n",
        "   # user_row = user_item_matrix.loc[user_id]\n",
        "   # user_similarities = pd.Series(cosine_sim[user_id])\n",
        "   # top_users = user_similarities.sort_values(ascending=False)[1:num_recommendations+1].index\n",
        "   # top_users_filtered = filtered_matrix[top_users]\n",
        "   # weighted_ratings = np.dot(top_users_filtered, user_row) / np.sum(top_users_filtered, axis=1)\n",
        "   # top_books = weighted_ratings.argsort()[::-1][:num_recommendations]\n",
        "   # return books_df.loc[top_books]\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations = recommend_books(user_id=500, num_recommendations=10)\n",
        "print(recommendations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGZ1Cw-KoDYy",
        "outputId": "65ca6483-99e1-48af-9ad3-6fee76ff0f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       book_id                               bookId  \\\n",
            "1697      1775       2247142.The_Talented_Mr_Ripley   \n",
            "8804      9704         6750.The_Broom_of_the_System   \n",
            "14511    16438                   24910.World_s_Fair   \n",
            "29694    39317  18485945-the-destiny-of-violet-luke   \n",
            "6485      7038       23961.Marabou_Stork_Nightmares   \n",
            "4965      5339     18667779-everything-leads-to-you   \n",
            "2258      2373                 15768191-sweet-peril   \n",
            "7138      7779                  30016.The_Naked_Sun   \n",
            "30864    40924                    7884898-intrigues   \n",
            "18953    23468        35197947-nekomonogatari-white   \n",
            "\n",
            "                                  title                             series  \\\n",
            "1697            The Talented Mr. Ripley                          Ripley #1   \n",
            "8804            The Broom of the System                                NaN   \n",
            "14511                      World's Fair                                NaN   \n",
            "29694      The Destiny of Violet & Luke                 The Coincidence #3   \n",
            "6485           Marabou Stork Nightmares                                NaN   \n",
            "4965            Everything Leads to You                                NaN   \n",
            "2258                        Sweet Peril                           Sweet #2   \n",
            "7138                      The Naked Sun                           Robot #2   \n",
            "30864                         Intrigues  Valdemar: Collegium Chronicles #2   \n",
            "18953  NEKOMONOGATARI (WHITE): Cat Tale              Monogatari #4, Part 2   \n",
            "\n",
            "                              author  rating  \\\n",
            "1697              Patricia Highsmith    3.93   \n",
            "8804            David Foster Wallace    3.84   \n",
            "14511                  E.L. Doctorow    3.83   \n",
            "29694              Jessica Sorensen     4.11   \n",
            "6485                    Irvine Welsh    3.87   \n",
            "4965                    Nina LaCour     3.86   \n",
            "2258                  Wendy Higgins     4.26   \n",
            "7138                    Isaac Asimov    4.16   \n",
            "30864                Mercedes Lackey    3.98   \n",
            "18953  NisiOisiN, VOFAN , Ko Ransom     4.49   \n",
            "\n",
            "                                             description language  \\\n",
            "1697   Since his debut in 1955, Tom Ripley has evolve...  English   \n",
            "8804   Published when Wallace was just twenty-four ye...  English   \n",
            "14511  The astonishing novel of a young boy's life in...  English   \n",
            "29694  Luke Price's life has always been about order,...  English   \n",
            "6485   The acclaimed author of the cult classics Trai...  English   \n",
            "4965   A love letter to the craft and romance of film...  English   \n",
            "2258   Anna Whitt, the daughter of a guardian angel a...  English   \n",
            "7138   A millennium into the future, two advancements...  English   \n",
            "30864  \"Spellbinding storyteller\" (Rave Reviews) Merc...  English   \n",
            "18953  Launching into new territory that the author h...  English   \n",
            "\n",
            "                isbn                                             genres  ...  \\\n",
            "1697   9780393332148  ['Fiction', 'Mystery', 'Thriller', 'Crime', 'C...  ...   \n",
            "8804   9780142002421  ['Fiction', 'Novels', 'Literature', 'Contempor...  ...   \n",
            "14511  9780452275720  ['Fiction', 'Historical Fiction', 'New York', ...  ...   \n",
            "29694  9781455576517  ['New Adult', 'Romance', 'Contemporary', 'Coll...  ...   \n",
            "6485   9780393315639  ['Fiction', 'Contemporary', 'Novels', 'Literat...  ...   \n",
            "4965   9780525425885  ['Young Adult', 'LGBT', 'Contemporary', 'Roman...  ...   \n",
            "2258   9780062265944  ['Fantasy', 'Young Adult', 'Paranormal', 'Roma...  ...   \n",
            "7138   9780586010167  ['Science Fiction', 'Fiction', 'Mystery', 'Rob...  ...   \n",
            "30864  9780756406394  ['Fantasy', 'Fiction', 'Young Adult', 'High Fa...  ...   \n",
            "18953  9781945054495  ['Light Novel', 'Fantasy', 'Manga', 'Comedy', ...  ...   \n",
            "\n",
            "          firstPublishDate                                             awards  \\\n",
            "1697              10/28/55  ['Grand Prix de LittÃ©rature PoliciÃ¨re for Ro...   \n",
            "8804   1987-06-01 00:00:00                                                 []   \n",
            "14511  1985-12-10 00:00:00         ['National Book Award for Fiction (1986)']   \n",
            "29694  2014-07-01 00:00:00                                                 []   \n",
            "6485              10/28/95                                                 []   \n",
            "4965                   NaN  ['Goodreads Choice Award Nominee for Young Adu...   \n",
            "2258                   NaN                                                 []   \n",
            "7138              12/28/56                                                 []   \n",
            "30864  2010-01-10 00:00:00                                                 []   \n",
            "18953             10/27/10                                                 []   \n",
            "\n",
            "      numRatings                               ratingsByStars likedPercent  \\\n",
            "1697       61428  ['18545', '25787', '12737', '3055', '1304']         93.0   \n",
            "8804       19198      ['5005', '8001', '4633', '1238', '321']         92.0   \n",
            "14511       4018        ['1011', '1660', '1054', '229', '64']         93.0   \n",
            "29694      12390       ['4870', '4769', '2168', '447', '136']         95.0   \n",
            "6485       10123       ['3069', '3685', '2542', '666', '161']         92.0   \n",
            "4965       28361      ['9311', '9608', '6533', '1917', '992']         90.0   \n",
            "2258       45250    ['22655', '14090', '6467', '1505', '533']         95.0   \n",
            "7138       43245      ['16216', '18840', '7327', '766', '96']         98.0   \n",
            "30864       8354        ['2875', '2999', '1983', '417', '80']         94.0   \n",
            "18953        447               ['268', '138', '35', '6', '0']         99.0   \n",
            "\n",
            "                                                 setting  \\\n",
            "1697   ['Italy', 'New York City, New York (United Sta...   \n",
            "8804                 ['Cleveland, Ohio (United States)']   \n",
            "14511        ['New York City, New York (United States)']   \n",
            "29694                                                 []   \n",
            "6485             ['Edinburgh, Scotland', 'South Africa']   \n",
            "4965              ['Venice, California (United States)']   \n",
            "2258                                                  []   \n",
            "7138                                                  []   \n",
            "30864                                                 []   \n",
            "18953                                                 []   \n",
            "\n",
            "                                                coverImg bbeScore bbeVotes  \\\n",
            "1697   https://i.gr-assets.com/images/S/compressed.ph...     2394       31   \n",
            "8804   https://i.gr-assets.com/images/S/compressed.ph...      264        3   \n",
            "14511  https://i.gr-assets.com/images/S/compressed.ph...      141        2   \n",
            "29694  https://i.gr-assets.com/images/S/compressed.ph...       84        1   \n",
            "6485   https://i.gr-assets.com/images/S/compressed.ph...      379        4   \n",
            "4965   https://i.gr-assets.com/images/S/compressed.ph...      548        6   \n",
            "2258   https://i.gr-assets.com/images/S/compressed.ph...     1588       22   \n",
            "7138   https://i.gr-assets.com/images/S/compressed.ph...      339        5   \n",
            "30864  https://i.gr-assets.com/images/S/compressed.ph...       80        1   \n",
            "18953  https://i.gr-assets.com/images/S/compressed.ph...       99        1   \n",
            "\n",
            "      price  \n",
            "1697   9.12  \n",
            "8804   7.14  \n",
            "14511  4.81  \n",
            "29694  8.02  \n",
            "6485   3.74  \n",
            "4965   6.51  \n",
            "2258   3.11  \n",
            "7138   3.61  \n",
            "30864  5.08  \n",
            "18953   8.8  \n",
            "\n",
            "[10 rows x 35 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a24bb1c3deb7>:113: RuntimeWarning: invalid value encountered in true_divide\n",
            "  weighted_ratings = np.dot(top_users_filtered, user_row) / np.sum(top_users_filtered, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GSyrropzpXxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}