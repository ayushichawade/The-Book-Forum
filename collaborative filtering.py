# -*- coding: utf-8 -*-
"""Untitled42.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aFfBqNatURSuKbTbpH4Yp105fGMJxHIV
"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, GRU, LSTM
from keras.models import Model
from keras import regularizers, optimizers
from keras.metrics import mean_absolute_error
from keras.metrics import MeanSquaredError
from sklearn.metrics import mean_squared_error

from keras.layers import Input, Dense, Dropout, Conv1D, MaxPooling1D, GRU, LSTM, SeparableConv1D

books_df = pd.read_excel('books_1.Best_Books_Ever.csv.xlsx')
users_df = pd.read_csv('goodreads_interactions (1).csv')

# Merge dataframes
merged_df = pd.merge(users_df, books_df, on='book_id')

# One-hot encode genre
genres = pd.get_dummies(merged_df['genre 1'])
merged_df = pd.concat([merged_df, genres], axis=1)

# Create user-item matrix
user_item_matrix = pd.pivot_table(merged_df, values='is_reviewed', index='user_id', columns='book_id')
user_item_matrix = user_item_matrix.fillna(0)

# Compute cosine similarity matrix
cosine_sim = cosine_similarity(user_item_matrix)

# Define autoencoder model
input_layer = Input(shape=(user_item_matrix.shape[1],))
encoded = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(input_layer)
encoded = Dropout(0.5)(encoded)
encoded = Dense(256, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(encoded)
encoded = Dropout(0.5)(encoded)
encoded = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(10e-5))(encoded)
encoded = Dropout(0.5)(encoded)

encoded = encoded[:, :, np.newaxis]  # add new axis for channels
encoded = SeparableConv1D(64, 3, activation='relu', padding='same')(encoded)

#encoded = Conv1D(64, 3, activation='relu', padding='same')(encoded)
encoded = MaxPooling1D(2)(encoded)
encoded = GRU(32, activation='relu', return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(encoded)
encoded = LSTM(32, activation='relu', dropout=0.2, recurrent_dropout=0.2)(encoded)

encoded = encoded[:, :, np.newaxis]  # add new axis for channels
encoded = SeparableConv1D(64, 3, activation='relu', padding='same')(encoded)

encoded = Conv1D(32, 5, activation='relu', padding='same')(encoded)
encoded = MaxPooling1D(2)(encoded)

encoded = GRU(32, activation='relu', return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(encoded)
encoded = LSTM(32, activation='relu', dropout=0.2, recurrent_dropout=0.2)(encoded)


decoded = Dense(128, activation='relu')(encoded)
decoded = Dropout(0.5)(decoded)
decoded = Dense(256, activation='relu')(decoded)
decoded = Dropout(0.5)(decoded)
decoded = Dense(512, activation='relu')(decoded)
decoded = Dropout(0.5)(decoded)
decoded = Dense(user_item_matrix.shape[1], activation='sigmoid')(decoded)
autoencoder = Model(input_layer, decoded)

# Compile autoencoder model
optimizer = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
autoencoder.compile(optimizer=optimizer, loss='mean_squared_error')

# Train autoencoder
history = autoencoder.fit(user_item_matrix, user_item_matrix, epochs=10, batch_size=128, validation_split=0.2)



# Predict filtered matrix
filtered_matrix = autoencoder.predict(user_item_matrix)

filtered_matrix = np.nan_to_num(filtered_matrix)
filtered_cosine_sim = cosine_similarity(filtered_matrix)


import numpy as np

def mean_absolute_percentage_error(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# Calculate MSE, MAE, and MAPE
mse = mean_squared_error(user_item_matrix, filtered_matrix)
mae = mean_absolute_error(user_item_matrix, filtered_matrix)
mape = mean_absolute_percentage_error(user_item_matrix, filtered_matrix)
print("MSE:", mse)
#print("MAE:", mae)
#print("MAPE:", mape)

import numpy as np

mae_value = np.mean(mae.numpy())
print("MAE:", mae_value)

rmse = np.sqrt(mse)
print("RMSE:", rmse)



import random
def recommend_books(user_id, num_recommendations=50):
    user_row = user_item_matrix.loc[user_id]
    user_similarities = pd.Series(cosine_sim[user_id])
    top_users = user_similarities.sort_values(ascending=False)[1:num_recommendations+1].index
    top_users_filtered = filtered_matrix[top_users]
    weighted_ratings = np.dot(top_users_filtered, user_row) / np.sum(top_users_filtered, axis=1)
    books_read = user_row[user_row > 0].index
    books_unread = books_df[~books_df['book_id'].isin(books_read)]['book_id']
    recommended_books = [book_id for book_id in books_unread if book_id in books_df.index]
    recommended_books = random.sample(recommended_books, min(num_recommendations, len(recommended_books)))
    return books_df.loc[recommended_books]


    recommendations = recommend_books(user_id=500, num_recommendations=10)
    print(recommendations)